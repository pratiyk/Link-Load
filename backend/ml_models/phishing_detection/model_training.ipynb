{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d19b27f",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb012fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\python312\\lib\\site-packages (1.5.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Collecting xgboost\n",
      "  Downloading xgboost-3.0.2-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: joblib in c:\\python312\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\sendt\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\python312\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\python312\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Downloading xgboost-3.0.2-py3-none-win_amd64.whl (150.0 MB)\n",
      "   ---------------------------------------- 0.0/150.0 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 10.5/150.0 MB 59.8 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 18.1/150.0 MB 47.5 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 23.6/150.0 MB 39.3 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 24.9/150.0 MB 30.3 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 26.5/150.0 MB 25.4 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 28.3/150.0 MB 23.0 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 30.7/150.0 MB 20.7 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 33.6/150.0 MB 19.7 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 36.4/150.0 MB 19.1 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 39.6/150.0 MB 18.9 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 43.5/150.0 MB 18.8 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 47.7/150.0 MB 18.9 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 52.2/150.0 MB 19.0 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 56.9/150.0 MB 19.3 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 61.3/150.0 MB 19.4 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 65.8/150.0 MB 19.5 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 70.8/150.0 MB 19.7 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 76.8/150.0 MB 20.2 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 82.6/150.0 MB 20.6 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 88.9/150.0 MB 21.1 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 94.6/150.0 MB 21.5 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 98.3/150.0 MB 21.3 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 102.2/150.0 MB 21.2 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 105.6/150.0 MB 20.9 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 108.3/150.0 MB 20.7 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 110.6/150.0 MB 20.4 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 113.5/150.0 MB 20.0 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 116.7/150.0 MB 19.9 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 120.1/150.0 MB 19.8 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 124.0/150.0 MB 19.7 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 128.2/150.0 MB 19.7 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 132.9/150.0 MB 19.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 137.4/150.0 MB 19.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 142.3/150.0 MB 20.0 MB/s eta 0:00:01\n",
      "   --------------------------------------  147.8/150.0 MB 20.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  149.9/150.0 MB 20.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  149.9/150.0 MB 20.2 MB/s eta 0:00:01\n",
      "   --------------------------------------- 150.0/150.0 MB 19.2 MB/s eta 0:00:00\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-3.0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ip (c:\\Python312\\Lib\\site-packages)\n",
      "DEPRECATION: Loading egg at c:\\python312\\lib\\site-packages\\vboxapi-1.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "WARNING: Ignoring invalid distribution ~ip (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (c:\\Python312\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn xgboost joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e16374ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\sendt\\appdata\\roaming\\python\\python312\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\sendt\\appdata\\roaming\\python\\python312\\site-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\python312\\lib\\site-packages (1.5.0)\n",
      "Requirement already satisfied: xgboost in c:\\python312\\lib\\site-packages (3.0.2)\n",
      "Collecting lightgbm\n",
      "  Downloading lightgbm-4.6.0-py3-none-win_amd64.whl.metadata (17 kB)\n",
      "Collecting imbalanced-learn\n",
      "  Downloading imbalanced_learn-0.13.0-py3-none-any.whl.metadata (8.8 kB)\n",
      "Requirement already satisfied: joblib in c:\\python312\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sendt\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sendt\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\python312\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\python312\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\python312\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Collecting sklearn-compat<1,>=0.1 (from imbalanced-learn)\n",
      "  Downloading sklearn_compat-0.1.3-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading lightgbm-4.6.0-py3-none-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 15.2 MB/s eta 0:00:00\n",
      "Downloading imbalanced_learn-0.13.0-py3-none-any.whl (238 kB)\n",
      "Downloading sklearn_compat-0.1.3-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: lightgbm, sklearn-compat, imbalanced-learn\n",
      "Successfully installed imbalanced-learn-0.13.0 lightgbm-4.6.0 sklearn-compat-0.1.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ip (C:\\Python312\\Lib\\site-packages)\n",
      "DEPRECATION: Loading egg at c:\\python312\\lib\\site-packages\\vboxapi-1.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Python312\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy scikit-learn xgboost lightgbm imbalanced-learn joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "909acc87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (84049, 15), Test shape: (21013, 15)\n",
      "\n",
      "Class distribution before SMOTE:\n",
      "1    42025\n",
      "0    42024\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class distribution after SMOTE:\n",
      "1    42025\n",
      "0    42025\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Starting model training...\n",
      "\n",
      "Training RandomForest...\n",
      "RandomForest results:\n",
      "   F1 (CV): 0.9945 Â± 0.0006\n",
      "   Test F1: 0.9947, ROC-AUC: 0.9982\n",
      "   Precision: 0.9979, Recall: 0.9914\n",
      "   Inference Time: 0.1637s\n",
      "\n",
      "Training XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:52:30] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost results:\n",
      "   F1 (CV): 0.9946 Â± 0.0006\n",
      "   Test F1: 0.9947, ROC-AUC: 0.9982\n",
      "   Precision: 0.9981, Recall: 0.9912\n",
      "   Inference Time: 0.0312s\n",
      "\n",
      "Training LogisticRegression...\n",
      "LogisticRegression results:\n",
      "   F1 (CV): 0.9913 Â± 0.0010\n",
      "   Test F1: 0.9914, ROC-AUC: 0.9973\n",
      "   Precision: 0.9982, Recall: 0.9848\n",
      "   Inference Time: 0.0220s\n",
      "\n",
      "Training GradientBoosting...\n",
      "GradientBoosting results:\n",
      "   F1 (CV): 0.9949 Â± 0.0007\n",
      "   Test F1: 0.9947, ROC-AUC: 0.9981\n",
      "   Precision: 0.9981, Recall: 0.9914\n",
      "   Inference Time: 0.1407s\n",
      "\n",
      "Training LightGBM...\n",
      "[LightGBM] [Info] Number of positive: 42025, number of negative: 42025\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002496 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 617\n",
      "[LightGBM] [Info] Number of data points in the train set: 84050, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "LightGBM results:\n",
      "   F1 (CV): 0.9945 Â± 0.0006\n",
      "   Test F1: 0.9946, ROC-AUC: 0.9982\n",
      "   Precision: 0.9980, Recall: 0.9911\n",
      "   Inference Time: 0.0432s\n",
      "\n",
      "Training SVM...\n",
      "SVM results:\n",
      "   F1 (CV): 0.9914 Â± 0.0010\n",
      "   Test F1: 0.9910, ROC-AUC: 0.9970\n",
      "   Precision: 0.9949, Recall: 0.9871\n",
      "   Inference Time: 8.6285s\n",
      "\n",
      "Model Comparison:\n",
      "                     roc_auc        f1 precision    recall cv_mean_f1\n",
      "LightGBM             0.99823  0.994556  0.997987  0.991148   0.994474\n",
      "XGBoost             0.998206  0.994651  0.998083  0.991243   0.994582\n",
      "RandomForest        0.998198  0.994652  0.997892  0.991433   0.994463\n",
      "GradientBoosting    0.998133  0.994747  0.998084  0.991433   0.994906\n",
      "LogisticRegression  0.997295  0.991424  0.998167  0.984771   0.991328\n",
      "SVM                 0.996986  0.990969  0.994915  0.987055   0.991363\n",
      "\n",
      "Best model: LightGBM\n",
      "ROC-AUC: 0.9982\n",
      "F1 Score: 0.9946\n",
      "Confusion Matrix:\n",
      "[[10486    21]\n",
      " [   93 10413]]\n",
      "\n",
      "Best model saved to: C:\\prateek\\projects\\linkload\\backend\\ml_models\\phishing_detection\\phishing_detector_model.pkl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import joblib\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import (accuracy_score, f1_score, roc_auc_score, \n",
    "                            precision_score, recall_score, confusion_matrix)\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# --- Load processed data ---\n",
    "DATA_DIR = r\"C:\\prateek\\projects\\linkload\\backend\\ml_models\\phishing_detection\\data\"\n",
    "\n",
    "X_train = pd.read_csv(f\"{DATA_DIR}/X_train.csv\")\n",
    "X_test = pd.read_csv(f\"{DATA_DIR}/X_test.csv\")\n",
    "y_train = pd.read_csv(f\"{DATA_DIR}/y_train.csv\").values.ravel()\n",
    "y_test = pd.read_csv(f\"{DATA_DIR}/y_test.csv\").values.ravel()\n",
    "\n",
    "print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
    "\n",
    "# --- Handle class imbalance ---\n",
    "print(\"\\nClass distribution before SMOTE:\")\n",
    "print(pd.Series(y_train).value_counts())\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"\\nClass distribution after SMOTE:\")\n",
    "print(pd.Series(y_train_res).value_counts())\n",
    "\n",
    "# --- Define candidate models with balanced class weights ---\n",
    "models = {\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    \"XGBoost\": XGBClassifier(\n",
    "        n_estimators=200,\n",
    "        scale_pos_weight=sum(y_train == 0)/sum(y_train == 1),  # Handle imbalance\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    \"LogisticRegression\": LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(\n",
    "        n_estimators=200,\n",
    "        random_state=42\n",
    "    ),\n",
    "    \"LightGBM\": LGBMClassifier(\n",
    "        n_estimators=200,\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    \"SVM\": SVC(\n",
    "        kernel='rbf',\n",
    "        class_weight='balanced',\n",
    "        probability=True,\n",
    "        random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "# --- Evaluation function ---\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"Evaluate model and return comprehensive metrics\"\"\"\n",
    "    start_time = time.time()\n",
    "    preds = model.predict(X_test)\n",
    "    proba = model.predict_proba(X_test)[:, 1]\n",
    "    end_time = time.time()\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(y_test, preds),\n",
    "        \"precision\": precision_score(y_test, preds),\n",
    "        \"recall\": recall_score(y_test, preds),\n",
    "        \"f1\": f1_score(y_test, preds),\n",
    "        \"roc_auc\": roc_auc_score(y_test, proba),\n",
    "        \"inference_time\": end_time - start_time,\n",
    "        \"confusion_matrix\": confusion_matrix(y_test, preds)\n",
    "    }\n",
    "\n",
    "# --- Train and evaluate with cross-validation ---\n",
    "results = {}\n",
    "trained_models = {}\n",
    "\n",
    "print(\"\\nStarting model training...\")\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(\n",
    "        model, X_train_res, y_train_res, \n",
    "        cv=5, scoring='f1', n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Train final model\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train_res, y_train_res)\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # Evaluate\n",
    "    metrics = evaluate_model(model, X_test, y_test)\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        \"cv_mean_f1\": np.mean(cv_scores),\n",
    "        \"cv_std_f1\": np.std(cv_scores),\n",
    "        \"train_time\": train_time,\n",
    "        **metrics\n",
    "    }\n",
    "    \n",
    "    trained_models[name] = model\n",
    "    \n",
    "    print(f\"{name} results:\")\n",
    "    print(f\"   F1 (CV): {results[name]['cv_mean_f1']:.4f} Â± {results[name]['cv_std_f1']:.4f}\")\n",
    "    print(f\"   Test F1: {results[name]['f1']:.4f}, ROC-AUC: {results[name]['roc_auc']:.4f}\")\n",
    "    print(f\"   Precision: {results[name]['precision']:.4f}, Recall: {results[name]['recall']:.4f}\")\n",
    "    print(f\"   Inference Time: {results[name]['inference_time']:.4f}s\")\n",
    "\n",
    "# --- Model selection ---\n",
    "print(\"\\nModel Comparison:\")\n",
    "results_df = pd.DataFrame(results).T.sort_values('roc_auc', ascending=False)\n",
    "print(results_df[['roc_auc', 'f1', 'precision', 'recall', 'cv_mean_f1']])\n",
    "\n",
    "# Select best model\n",
    "best_model_name = results_df.index[0]\n",
    "best_model = trained_models[best_model_name]\n",
    "\n",
    "print(f\"\\nBest model: {best_model_name}\")\n",
    "print(f\"ROC-AUC: {results[best_model_name]['roc_auc']:.4f}\")\n",
    "print(f\"F1 Score: {results[best_model_name]['f1']:.4f}\")\n",
    "print(f\"Confusion Matrix:\\n{results[best_model_name]['confusion_matrix']}\")\n",
    "\n",
    "# --- Save best model ---\n",
    "OUTPUT_MODEL_PATH = r\"C:\\prateek\\projects\\linkload\\backend\\ml_models\\phishing_detection\\phishing_detector_model.pkl\"\n",
    "joblib.dump(best_model, OUTPUT_MODEL_PATH)\n",
    "\n",
    "print(f\"\\nBest model saved to: {OUTPUT_MODEL_PATH}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
